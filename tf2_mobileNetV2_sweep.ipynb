{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/Event_Camera_MasterThesis/timeStack_data_1281281.zip ./\n",
    "!unzip ./timeStack_data_1281281.zip -d ./\n",
    "!cp /content/drive/MyDrive/Event_Camera_MasterThesis/timeStack_data_1281283.zip ./\n",
    "!unzip ./timeStack_data_1281283.zip -d ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s /content/drive/MyDrive/Event_Camera_MasterThesis/models models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from wandb.integration.keras import WandbCallback\n",
    "import wandb\n",
    "from collections import Counter\n",
    "\n",
    "# 自定义 WandbCallback，覆盖 on_train_batch_end 以跳过图记录\n",
    "class CustomWandbCallback(WandbCallback):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._step = 0\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        wandb.log(logs, step=self._step)\n",
    "        self._step += 1\n",
    "\n",
    "\n",
    "def load_pt_file(file_path):\n",
    "    \"\"\"\n",
    "    加载 .pt 文件，返回一个 numpy 数组，假设数据已经为 (128, 128, 3) 格式\n",
    "    \"\"\"\n",
    "    tensor = torch.load(file_path)\n",
    "    np_array = tensor.numpy()\n",
    "    return np_array\n",
    "\n",
    "def parse_function(file_path, table, num_classes):\n",
    "    def _load_and_resize(fp):\n",
    "        fp_str = fp.numpy().decode('utf-8')\n",
    "        img = load_pt_file(fp_str)\n",
    "        # 将图像 resize 到 224×224\n",
    "        img_resized = tf.image.resize(img, (224, 224))\n",
    "        return img_resized\n",
    "    img = tf.py_function(func=_load_and_resize, inp=[file_path], Tout=tf.float32)\n",
    "    # 更新输出形状为 (224,224,3)\n",
    "    img.set_shape([224, 224, 3])\n",
    "    img = img / 255.0\n",
    "\n",
    "    # 提取标签：假设文件路径结构为 .../class_name/filename.pt\n",
    "    parts = tf.strings.split(file_path, os.sep)\n",
    "    label_str = parts[-2]\n",
    "    label_int = table.lookup(label_str)\n",
    "    label = tf.one_hot(label_int, depth=num_classes)\n",
    "    return img, label\n",
    "\n",
    "\n",
    "def create_dataset(data_dir, batch_size, seed=42):\n",
    "    # 获取所有 .pt 文件，并过滤掉包含 \".ipynb_checkpoints\" 的文件\n",
    "    all_files = glob.glob(os.path.join(data_dir, \"*/*.pt\"))\n",
    "    all_files = [f for f in all_files if \".ipynb_checkpoints\" not in f]\n",
    "    \n",
    "    # 获取有效类别列表（从父文件夹名称获取），并排序\n",
    "    valid_classes = sorted([cls for cls in os.listdir(data_dir) \n",
    "                             if os.path.isdir(os.path.join(data_dir, cls)) and cls != '.ipynb_checkpoints'])\n",
    "    \n",
    "    print(\"有效类别:\", valid_classes)\n",
    "    \n",
    "    # 构造类别映射查找表\n",
    "    keys = tf.constant(valid_classes)\n",
    "    vals = tf.constant(range(len(valid_classes)), dtype=tf.int32)\n",
    "    table = tf.lookup.StaticHashTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(keys, vals), default_value=-1)\n",
    "    num_classes = len(valid_classes)\n",
    "    \n",
    "    # 获取每个文件对应的类别（父目录名称）\n",
    "    labels = [os.path.basename(os.path.dirname(f)) for f in all_files]\n",
    "    \n",
    "    # stratified 分层划分\n",
    "    train_files, val_files = train_test_split(\n",
    "        all_files, test_size=0.3, random_state=seed, stratify=labels)\n",
    "    \n",
    "    print(\"训练集样本数量:\", len(train_files))\n",
    "    print(\"验证集样本数量:\", len(val_files))\n",
    "    print(\"训练集类别分布:\", Counter([os.path.basename(os.path.dirname(f)) for f in train_files]))\n",
    "    print(\"验证集类别分布:\", Counter([os.path.basename(os.path.dirname(f)) for f in val_files]))\n",
    "    \n",
    "    # 构建 tf.data.Dataset\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(train_files)\n",
    "    train_ds = train_ds.map(lambda x: parse_function(x, table, num_classes), \n",
    "                             num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds = train_ds.shuffle(1000, seed=seed).batch(batch_size, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_ds = tf.data.Dataset.from_tensor_slices(val_files)\n",
    "    val_ds = val_ds.map(lambda x: parse_function(x, table, num_classes), \n",
    "                         num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.batch(batch_size, drop_remainder=False).prefetch(tf.data.AUTOTUNE)\n",
    "    return train_ds, val_ds\n",
    "\n",
    "def train():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    seed = 42\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    data_paths = {\n",
    "        \"timeStack1281281\": \"/content/timeStack_data_1281281\",\n",
    "        \"timeStack1281283\": \"/content/timeStack_data_1281283\"\n",
    "    }\n",
    "    data_name = config.get(\"data_name\", \"timeStack1281281\")\n",
    "    learning_rate = config.get(\"learning_rate\", 0.0001)\n",
    "    epochs = config.get(\"epochs\", 90)\n",
    "    batch_size = config.get(\"batch_size\", 16)\n",
    "    patience = config.get(\"patience\", 100)\n",
    "    min_delta = config.get(\"min_delta\", 0.01)\n",
    "    \n",
    "    train_dataset, val_dataset = create_dataset(data_paths[data_name], batch_size, seed)\n",
    "    \n",
    "    input_tensor = Input(shape=(224, 224, 3))\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "    base_model.trainable = True\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < 20:\n",
    "            return learning_rate * (epoch + 1) / 20\n",
    "        elif epoch < 80:\n",
    "            T = 60\n",
    "            cos_inner = np.pi * (epoch - 20) / T\n",
    "            return learning_rate * (np.cos(cos_inner) + 1) / 2\n",
    "        else:\n",
    "            return learning_rate * 0.01\n",
    "    lr_callback = LearningRateScheduler(scheduler)\n",
    "    \n",
    "    os.makedirs(\"/content/models\", exist_ok=True)\n",
    "    checkpoint_path = f\"/content/models/best_model_{data_name}_mobileNetV2.h5\"\n",
    "    early_stop = EarlyStopping(monitor='val_loss', min_delta=min_delta, patience=patience, restore_best_weights=True)\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    \n",
    "    # 使用自定义的 WandbCallback，禁用模型图记录\n",
    "    wandb_callback = CustomWandbCallback(save_model=False, log_graph=False)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[lr_callback, early_stop, checkpoint, wandb_callback]\n",
    "    )\n",
    "    \n",
    "    eval_results = model.evaluate(val_dataset)\n",
    "    wandb.log({\"Test Loss\": eval_results[0], \"Test Accuracy\": eval_results[1]*100})\n",
    "    \n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    for images, labels in val_dataset:\n",
    "        preds = model.predict(images)\n",
    "        y_preds.extend(np.argmax(preds, axis=1))\n",
    "        y_trues.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    cm = metrics.confusion_matrix(y_trues, y_preds)\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                          display_labels=[str(i) for i in range(10)])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(\"confusion_matrix.png\")\n",
    "    wandb.log({\"Confusion Matrix\": wandb.Image(\"confusion_matrix.png\")})\n",
    "    \n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第三步：定义 sweep 配置\n",
    "sweep_config = {\n",
    "    \"program\": \"train.py\",  # 这里可不影响 agent 运行我们的 train() 函数\n",
    "    \"method\": \"grid\",  # 可选 \"grid\", \"random\", \"bayes\" 等\n",
    "    \"metric\": {\n",
    "        \"name\": \"val_loss\",\n",
    "        \"goal\": \"minimize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"data_name\": {\n",
    "            \"values\": [\"timeStack1281281\", \"timeStack1281283\"]\n",
    "        },\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [1e-4, 3e-4, 1e-5]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"value\": 90\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [16, 32]\n",
    "        },\n",
    "        \"patience\": {\n",
    "            \"value\": 100\n",
    "        },\n",
    "        \"min_delta\": {\n",
    "            \"value\": 0.01\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 创建 sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"event_MT_tf2mobileNetV2_sweep\")\n",
    "print(\"Created sweep with ID:\", sweep_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第四步：启动 sweep agent\n",
    "# count 参数表示运行的试验次数，例如 4 次\n",
    "wandb.agent(sweep_id, function=train, count=12)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
